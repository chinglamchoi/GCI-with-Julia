{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare SentimentAnalysis models: Comparison with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing\n",
    "From the previously generated train_Y and test_Y files, I replace positive sentiment labels \"2\" with \"pos\", negative sentiment labels \"1\" with \"neg\", in order to follow sklearn convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = open(\"train_X.txt\", \"r\", encoding=\"iso-8859-1\")\n",
    "labels_f = open(\"train_YY.txt\", \"r\", encoding=\"iso-8859-1\")\n",
    "test_fd = open(\"test_X.txt\", \"r\", encoding=\"iso-8859-1\")\n",
    "test_fl = open(\"test_YY.txt\", \"r\", encoding=\"iso-8859-1\")\n",
    "\n",
    "def f(x):\n",
    "    j = []\n",
    "    for i in x:\n",
    "        if len(i) > 102: #min length of both trainset & testset, sklearn requires same feature dims\n",
    "            j.append(i[:102])\n",
    "        else:\n",
    "            j.append(i)\n",
    "    return j\n",
    "\n",
    "train_data = np.array(f(data_f))\n",
    "train_labels = np.array(f(labels_f))\n",
    "test_data = np.array(f(test_fd))\n",
    "test_labels = np.array(f(test_fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model definition & Training\n",
    "As this is a binary classification task involving 2 labels: positive/negative, I use the logsitic regression model. Due to Out of Memory issues, I conduct training on a 12000-sample larger subset of the trainset, with a maximum of 200 uterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(max_iter=200)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            lowercase = False)\n",
    "\n",
    "batch = 4000\n",
    "iterr = 100\n",
    "\n",
    "cnt = 0\n",
    "total = len(test_labels)\n",
    "correct = 0\n",
    "\n",
    "features1 = vectorizer.fit_transform(train_data[:batch*3])\n",
    "features1 = features1.toarray()\n",
    "log_model = log_model.fit(X=features1, y=train_labels[:batch*3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing\n",
    "To avoid OOM issues, I conduct inference of batches of 4000 across in entire testset over 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Correct: 2030/4000\n",
      "[2] Correct: 4113/8000\n",
      "[3] Correct: 6163/12000\n",
      "[4] Correct: 8201/16000\n",
      "[5] Correct: 10259/20000\n",
      "[6] Correct: 12352/24000\n",
      "[7] Correct: 14408/28000\n",
      "[8] Correct: 16367/32000\n",
      "[9] Correct: 18342/36000\n",
      "[10] Correct: 20364/40000\n",
      "[11] Correct: 22315/44000\n",
      "[12] Correct: 24342/48000\n",
      "[13] Correct: 26331/52000\n",
      "[14] Correct: 28338/56000\n",
      "[15] Correct: 30261/60000\n",
      "[16] Correct: 32312/64000\n",
      "[17] Correct: 34360/68000\n",
      "[18] Correct: 36371/72000\n",
      "[19] Correct: 38448/76000\n",
      "[20] Correct: 40474/80000\n",
      "[21] Correct: 42503/84000\n",
      "[22] Correct: 44411/88000\n",
      "[23] Correct: 46458/92000\n",
      "[24] Correct: 48425/96000\n",
      "[25] Correct: 50372/100000\n",
      "[26] Correct: 52358/104000\n",
      "[27] Correct: 54434/108000\n",
      "[28] Correct: 56397/112000\n",
      "[29] Correct: 58342/116000\n",
      "[30] Correct: 60363/120000\n",
      "[31] Correct: 62467/124000\n",
      "[32] Correct: 64422/128000\n",
      "[33] Correct: 66352/132000\n",
      "[34] Correct: 68323/136000\n",
      "[35] Correct: 70294/140000\n",
      "[36] Correct: 72328/144000\n",
      "[37] Correct: 74362/148000\n",
      "[38] Correct: 76329/152000\n",
      "[39] Correct: 78208/156000\n",
      "[40] Correct: 80246/160000\n",
      "[41] Correct: 82248/164000\n",
      "[42] Correct: 84170/168000\n",
      "[43] Correct: 86118/172000\n",
      "[44] Correct: 88132/176000\n",
      "[45] Correct: 90080/180000\n",
      "[46] Correct: 92056/184000\n",
      "[47] Correct: 93922/188000\n",
      "[48] Correct: 95896/192000\n",
      "[49] Correct: 97887/196000\n",
      "[50] Correct: 99937/200000\n",
      "[51] Correct: 101829/204000\n",
      "[52] Correct: 103792/208000\n",
      "[53] Correct: 105794/212000\n",
      "[54] Correct: 107795/216000\n",
      "[55] Correct: 109679/220000\n",
      "[56] Correct: 111585/224000\n",
      "[57] Correct: 113629/228000\n",
      "[58] Correct: 115578/232000\n",
      "[59] Correct: 117531/236000\n",
      "[60] Correct: 119491/240000\n",
      "[61] Correct: 121469/244000\n",
      "[62] Correct: 123317/248000\n",
      "[63] Correct: 125289/252000\n",
      "[64] Correct: 127244/256000\n",
      "[65] Correct: 129269/260000\n",
      "[66] Correct: 131210/264000\n",
      "[67] Correct: 133197/268000\n",
      "[68] Correct: 135107/272000\n",
      "[69] Correct: 137088/276000\n",
      "[70] Correct: 139000/280000\n",
      "[71] Correct: 140960/284000\n",
      "[72] Correct: 143005/288000\n",
      "[73] Correct: 145004/292000\n",
      "[74] Correct: 146932/296000\n",
      "[75] Correct: 148841/300000\n",
      "[76] Correct: 150824/304000\n",
      "[77] Correct: 152784/308000\n",
      "[78] Correct: 154777/312000\n",
      "[79] Correct: 156765/316000\n",
      "[80] Correct: 158734/320000\n",
      "[81] Correct: 160628/324000\n",
      "[82] Correct: 162614/328000\n",
      "[83] Correct: 164579/332000\n",
      "[84] Correct: 166520/336000\n",
      "[85] Correct: 168426/340000\n",
      "[86] Correct: 170337/344000\n",
      "[87] Correct: 172336/348000\n",
      "[88] Correct: 174310/352000\n",
      "[89] Correct: 176283/356000\n",
      "[90] Correct: 178222/360000\n",
      "[91] Correct: 180137/364000\n",
      "[92] Correct: 182044/368000\n",
      "[93] Correct: 183967/372000\n",
      "[94] Correct: 185951/376000\n",
      "[95] Correct: 187892/380000\n",
      "[96] Correct: 189887/384000\n",
      "[97] Correct: 191838/388000\n",
      "[98] Correct: 193816/392000\n",
      "[99] Correct: 195792/396000\n",
      "[100] Correct: 197752/400000\n",
      "The final accuracy is:  0.49438\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    features2 = vectorizer.transform(test_data[cnt:cnt+batch])\n",
    "    features2 = features2.toarray()\n",
    "    cnt += batch\n",
    "    y_pred = log_model.predict(features2)\n",
    "    counter = 0\n",
    "    for j in y_pred:\n",
    "        if j == test_labels[counter]:\n",
    "            correct += 1\n",
    "    print(\"[\" + str(i+1) + \"] Correct: \" + str(correct) + \"/\" + str(4000*(i+1)))\n",
    "print(\"The final accuracy is: \", correct/total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite experimenting with different hyperparameters and settings, namely the number of epochs (100 and 200) and the trainset size (4000 vs. 8000 vs. 12000), accuracy of sklearn's logistic regression model plateaus at 49.4%. At 100 epochs, the accuracy on a training set of 4000 samples is 0.4647175; the accuracy on a trainset of 8000 is 0.493485; the accuracy on a trainset is 0.4944. Surprisingly, this accuracy decreases to 0.49438 when the epoch num is increased from 100 to 200. To allow for better accuracy, more complex architecture needs to be used, and training needs to be conducted with my samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. References\n",
    "1. [Making Sentiment Analysis Easy With Scikit-Learn] (https://www.twilio.com/blog/2017/12/sentiment-analysis-scikit-learn.html)\n",
    "2. [Sentiment Analysis â€” A how-to guide with movie reviews] (https://towardsdatascience.com/sentiment-analysis-a-how-to-guide-with-movie-reviews-9ae335e6bcb2)\n",
    "3. [Movie Reviews Sentiment Analysis with Scikit-Learn] (https://www.pitt.edu/~naraehan/presentation/Movie%20Reviews%20sentiment%20analysis%20with%20Scikit-Learn.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
